{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f41322",
   "metadata": {},
   "source": [
    "## Step 1: Load Model and Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(ds['test'])} test examples, {len(ds['train'])} train examples\")\n",
    "print(f\"Model loaded on device: {model.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device information\n",
    "print(\"=\"*60)\n",
    "print(\"DEVICE INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Memory information\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Check model device\n",
    "print(f\"\\nModel device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "\n",
    "# Check which device each parameter is on (for distributed models)\n",
    "devices = set()\n",
    "for name, param in model.named_parameters():\n",
    "    devices.add(str(param.device))\n",
    "\n",
    "if len(devices) > 1:\n",
    "    print(f\"\\nModel is distributed across multiple devices: {devices}\")\n",
    "else:\n",
    "    print(f\"\\nAll model parameters are on: {list(devices)[0]}\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdbc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.version.cuda); print(torch.backends.cudnn.version()); print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e697b4",
   "metadata": {},
   "source": [
    "## Step 2: Helper Functions for Answer Extraction and Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract the numerical answer from the text.\n",
    "    GSM8K answers typically end with #### followed by the number.\n",
    "    \"\"\"\n",
    "    # Try to find the answer after ####\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', text)\n",
    "    if match:\n",
    "        # Remove commas from the number\n",
    "        return match.group(1).replace(',', '')\n",
    "\n",
    "    # Fallback: try to find the last number in the text\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "\n",
    "    return None\n",
    "\n",
    "def check_answer_correct(generated_answer, reference_answer):\n",
    "    \"\"\"\n",
    "    Check if the generated answer matches the reference answer.\n",
    "    \"\"\"\n",
    "    gen = extract_answer(generated_answer)\n",
    "    ref = extract_answer(reference_answer)\n",
    "\n",
    "    if gen is None or ref is None:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Compare as floats to handle different formats\n",
    "        return abs(float(gen) - float(ref)) < 0.01\n",
    "    except:\n",
    "        return gen == ref\n",
    "\n",
    "# Test the extraction function\n",
    "test_answer = \"So the total is 50 + 30 = 80. #### 80\"\n",
    "print(f\"Test extraction: '{test_answer}' -> {extract_answer(test_answer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62963090",
   "metadata": {},
   "source": [
    "## Step 3: Generate Multiple Answers for a Question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(question, num_answers=10, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate multiple different answers to the same question using Qwen chat template.\n",
    "    \"\"\"\n",
    "    # Format the prompt with explicit instruction to use GSM8K format\n",
    "    prompt_text = f\"\"\"Question: {question}\n",
    "Answer: Let's solve this step by step concisely. End your answer with #### followed by the final numerical answer.\"\"\"\n",
    "\n",
    "    # Use Qwen chat template format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # Disable thinking mode for faster generation\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate multiple answers\n",
    "    answers = []\n",
    "    for i in range(num_answers):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Generating answer {i+1}/{num_answers}...\")\n",
    "        print('='*60)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode only the generated part (not the input prompt)\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "        generated_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        answers.append(generated_text)\n",
    "\n",
    "        # Print the result after each trial\n",
    "        print(f\"\\nGenerated Answer {i+1}:\")\n",
    "        print('-'*60)\n",
    "        print(generated_text )\n",
    "        print('-'*60)\n",
    "        extracted = extract_answer(generated_text)\n",
    "        print(f\"Extracted Answer: {extracted}\")\n",
    "\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460504fe",
   "metadata": {},
   "source": [
    "## Step 4: Generate and Verify Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the first question from the test set\n",
    "test_example = ds['test'][0]\n",
    "print(f\"Question: {test_example['question']}\")\n",
    "print(f\"\\nReference Answer: {test_example['answer']}\")\n",
    "print(f\"Reference Final Answer: {extract_answer(test_example['answer'])}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# Generate answers for the first question\n",
    "print(\"Generating different answers...\")\n",
    "num_answers = 3\n",
    "generated_answers = generate_answers(test_example['question'], num_answers=num_answers)\n",
    "\n",
    "# Check which answers are correct\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correct_answers = []\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    extracted = extract_answer(answer)\n",
    "    is_correct = check_answer_correct(answer, test_example['answer'])\n",
    "\n",
    "    print(f\"\\nAnswer {i+1}:\")\n",
    "    print(f\"Extracted value: {extracted}\")\n",
    "    print(f\"Correct: {is_correct}\")\n",
    "    print(f\"Response preview: {answer[:200]}...\")\n",
    "\n",
    "    if is_correct:\n",
    "        correct_answers.append(i)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Summary: {len(correct_answers)}/{num_answers} answers were correct\")\n",
    "print(f\"Correct answer indices: {correct_answers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e5ed0",
   "metadata": {},
   "source": [
    "## Step 5: Score Answers Using the LLM (Verifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answer(question, answer, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Use the LLM to score/verify an answer using chat template.\n",
    "    Returns the log probability or a confidence score.\n",
    "    \"\"\"\n",
    "    # Create a verification prompt using chat template\n",
    "    prompt_text = f\"\"\"Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Is this answer correct? Respond with yes or no.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Get the model's confidence by computing log probabilities\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**model_inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate the average log probability of the answer tokens\n",
    "        # This is a simple scoring mechanism\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the average log probability over all tokens in the sequence\n",
    "        input_ids = model_inputs['input_ids'][0]\n",
    "\n",
    "        if len(input_ids) > 1:\n",
    "            token_log_probs = []\n",
    "            for i in range(len(input_ids) - 1):\n",
    "                token_log_prob = log_probs[0, i, input_ids[i+1]].item()\n",
    "                token_log_probs.append(token_log_prob)\n",
    "\n",
    "            avg_log_prob = sum(token_log_probs) / len(token_log_probs) if token_log_probs else 0\n",
    "        else:\n",
    "            avg_log_prob = 0\n",
    "\n",
    "    return avg_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ad4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score all generated answers\n",
    "print(\"Scoring all generated answers...\")\n",
    "scores = []\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    score = score_answer(test_example['question'], answer)\n",
    "    scores.append(score)\n",
    "    is_correct = check_answer_correct(answer, test_example['answer'])\n",
    "    print(f\"Answer {i+1}: Score = {score:.4f}, Correct = {is_correct}\")\n",
    "\n",
    "# Find the best answer according to the verifier\n",
    "best_idx = scores.index(max(scores))\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Best answer according to verifier: Answer {best_idx + 1}\")\n",
    "print(f\"Is the best answer correct? {check_answer_correct(generated_answers[best_idx], test_example['answer'])}\")\n",
    "print(f\"Best answer: {generated_answers[best_idx][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb3e9f",
   "metadata": {},
   "source": [
    "## Step 6: Run Experiment on Multiple Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(num_questions=5, num_answers=10, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Run the complete experiment on multiple questions.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'questions': [],\n",
    "        'base_correct': [],  # First answer correct?\n",
    "        'any_correct': [],   # Any of the 10 correct?\n",
    "        'best_of_n_correct': [],  # Best-of-N answer correct?\n",
    "        'num_correct': []    # How many out of 10 were correct?\n",
    "    }\n",
    "\n",
    "    for q_idx in range(num_questions):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing Question {q_idx + 1}/{num_questions}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        example = ds['test'][q_idx]\n",
    "        question = example['question']\n",
    "        reference = example['answer']\n",
    "\n",
    "        print(f\"Question: {question[:100]}...\")\n",
    "\n",
    "        # Generate multiple answers\n",
    "        answers = generate_answers(question, num_answers=num_answers, max_new_tokens=max_new_tokens)\n",
    "\n",
    "        # Check correctness\n",
    "        correctness = [check_answer_correct(ans, reference) for ans in answers]\n",
    "\n",
    "        # Score answers\n",
    "        print(\"\\nScoring answers...\")\n",
    "        answer_scores = [score_answer(question, ans) for ans in answers]\n",
    "\n",
    "        # Get best answer\n",
    "        best_idx = answer_scores.index(max(answer_scores))\n",
    "\n",
    "        # Store results\n",
    "        results['questions'].append(question)\n",
    "        results['base_correct'].append(correctness[0])\n",
    "        results['any_correct'].append(any(correctness))\n",
    "        results['best_of_n_correct'].append(correctness[best_idx])\n",
    "        results['num_correct'].append(sum(correctness))\n",
    "\n",
    "        print(f\"\\nResults for Question {q_idx + 1}:\")\n",
    "        print(f\"  - Base model (first answer): {'✓' if correctness[0] else '✗'}\")\n",
    "        print(f\"  - Any correct: {'✓' if any(correctness) else '✗'}\")\n",
    "        print(f\"  - Best-of-{num_answers}: {'✓' if correctness[best_idx] else '✗'}\")\n",
    "        print(f\"  - Total correct: {sum(correctness)}/{num_answers}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "print(\"Starting experiment...\")\n",
    "experiment_results = run_experiment(num_questions=5, num_answers=10, max_new_tokens=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3460c",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03286206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "base_accuracy = sum(experiment_results['base_correct']) / len(experiment_results['base_correct'])\n",
    "best_of_n_accuracy = sum(experiment_results['best_of_n_correct']) / len(experiment_results['best_of_n_correct'])\n",
    "any_correct_rate = sum(experiment_results['any_correct']) / len(experiment_results['any_correct'])\n",
    "avg_correct = sum(experiment_results['num_correct']) / len(experiment_results['num_correct'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of questions tested: {len(experiment_results['questions'])}\")\n",
    "print(f\"\\nBase model accuracy (first answer): {base_accuracy:.2%}\")\n",
    "print(f\"Best-of-10 accuracy (verifier-selected): {best_of_n_accuracy:.2%}\")\n",
    "print(f\"Oracle best-of-10 (any correct): {any_correct_rate:.2%}\")\n",
    "print(f\"Average correct answers per question: {avg_correct:.2f}/10\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESEARCH QUESTION INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Q: Are answers with higher scores more likely to be correct?\")\n",
    "print(f\"A: Best-of-10 accuracy ({best_of_n_accuracy:.2%}) vs Base accuracy ({base_accuracy:.2%})\")\n",
    "print(f\"   Improvement: {best_of_n_accuracy - base_accuracy:+.2%}\")\n",
    "print(f\"\\nQ: Is the average best-of-10 answer more accurate than the average base answer?\")\n",
    "print(f\"A: {'YES' if best_of_n_accuracy > base_accuracy else 'NO'} - Best-of-10 is {best_of_n_accuracy/base_accuracy:.2f}x better\" if base_accuracy > 0 else \"A: Need non-zero base accuracy to compare\")\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED BREAKDOWN BY QUESTION:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(len(experiment_results['questions'])):\n",
    "    print(f\"\\nQ{i+1}: {experiment_results['questions'][i][:60]}...\")\n",
    "    print(f\"  Base: {'✓' if experiment_results['base_correct'][i] else '✗'} | \"\n",
    "          f\"Best-of-10: {'✓' if experiment_results['best_of_n_correct'][i] else '✗'} | \"\n",
    "          f\"Correct: {experiment_results['num_correct'][i]}/10\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
