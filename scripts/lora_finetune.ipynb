{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8559a81",
   "metadata": {},
   "source": [
    "# Step 1: Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0affddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from tqdm import tqdm \n",
    "\n",
    "MODEL_NAME = \"/Volumes/SSD/llm-projects/hugging_face/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca\"\n",
    "DATASET_PATH = \"verifier_dataset_train.json\"\n",
    "OUTPUT_DIR = \"./verifier_lora\"\n",
    "CHECKPOINT_FILE = \"training_state.pt\" \n",
    "\n",
    "\n",
    "MAX_LENGTH = 256             \n",
    "BATCH_SIZE = 2               \n",
    "GRAD_ACCUMULATION_STEPS = 16  \n",
    "\n",
    "\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-3 #5e-4\n",
    "DEBUG_SAMPLE_SIZE = 1000 # Set to None for full run\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b21f6",
   "metadata": {},
   "source": [
    "# Step 2: Tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45039edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from verifier_dataset_train.json...\n",
      "Train Samples: 28800 | Val Samples: 3200\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class VerifierDataset(Dataset):\n",
    "    def __init__(self, raw_data_list, tokenizer, max_length=512):\n",
    "        self.samples = []\n",
    "        for entry in raw_data_list:\n",
    "            question = entry['question']\n",
    "            answers = entry['answers']\n",
    "            labels = entry['answer_labels']\n",
    "            for ans, label in zip(answers, labels):\n",
    "                text = f\"Question: {question}\\nAnswer: {ans}\"\n",
    "                # Label must be float for BCE Loss (0.0 or 1.0)\n",
    "                self.samples.append({\"text\": text, \"label\": float(label)})\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False \n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"],\n",
    "            \"attention_mask\": encodings[\"attention_mask\"],\n",
    "            \"labels\": torch.tensor(item[\"label\"], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "print(f\"Loading data from {DATASET_PATH}...\")\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"Error: File not found.\")\n",
    "    raw_questions = []\n",
    "else:\n",
    "    with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "        raw_questions = json.load(f)\n",
    "\n",
    "if DEBUG_SAMPLE_SIZE: raw_questions = raw_questions[:DEBUG_SAMPLE_SIZE]\n",
    "random.seed(42)\n",
    "random.shuffle(raw_questions)\n",
    "\n",
    "split_idx = int(0.9 * len(raw_questions))\n",
    "if split_idx == 0 and len(raw_questions) > 0: split_idx = 1\n",
    "train_questions = raw_questions[:split_idx]\n",
    "val_questions = raw_questions[split_idx:]\n",
    "\n",
    "train_dataset = VerifierDataset(train_questions, tokenizer, MAX_LENGTH)\n",
    "val_dataset = VerifierDataset(val_questions, tokenizer, MAX_LENGTH)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator, pin_memory=True)\n",
    "\n",
    "print(f\"Train Samples: {len(train_dataset)} | Val Samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86564591",
   "metadata": {},
   "source": [
    "# Step 3: Model with LoRA and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /Volumes/SSD/llm-projects/hugging_face/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,147,904 || all params: 597,198,848 || trainable%: 0.1922\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "# num_labels=1 means we output a SINGLE number (The 'Logit')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1, \n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Apply LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1, #0.05\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    # use_dora=True #DoRA option, maybe for bigger model\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = len(train_loader) * EPOCHS // GRAD_ACCUMULATION_STEPS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss() # This is the Binary Cross Entropy Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a507d",
   "metadata": {},
   "source": [
    "# Step 4: Resume from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f607d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint: ./verifier_lora_adapter1/training_state.pt\n",
      "Loading state to resume training...\n",
      "Resuming from Epoch 3\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "checkpoint_path = os.path.join(OUTPUT_DIR, CHECKPOINT_FILE)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found checkpoint: {checkpoint_path}\")\n",
    "    print(\"Loading state to resume training...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load Model Weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # Load Optimizer Brain (Momentum)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # Load Schedule (Current Learning Rate)\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    # Load Epoch\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    \n",
    "    print(f\"Resuming from Epoch {start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c08e4",
   "metadata": {},
   "source": [
    "# Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c1ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/SSD/llm-projects/envs/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n",
      "Loading data from verifier_dataset_train.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 28800 | Val Samples: 3200\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /Volumes/SSD/llm-projects/hugging_face/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Volumes/SSD/llm-projects/envs/llm-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,147,904 || all params: 597,198,848 || trainable%: 0.1922\n",
      "No checkpoint found. Starting fresh.\n",
      "\n",
      "Starting Manual Training Loop...\n",
      "Effective Batch Size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 14400/14400 [5:28:48<00:00,  1.37s/it, loss=0.0287]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Epoch 1...\n",
      "Epoch 1 Finished | Train Loss: 0.3449 | Val Acc: 80.06%\n",
      "Saving Checkpoint...\n",
      "Checkpoint Saved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 14400/14400 [5:28:36<00:00,  1.37s/it, loss=0.0437]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2...\n",
      "Epoch 2 Finished | Train Loss: 0.1828 | Val Acc: 81.94%\n",
      "Saving Checkpoint...\n",
      "Checkpoint Saved.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:   1%|          | 84/14400 [02:17<5:41:22,  1.43s/it, loss=0.00189] "
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting Manual Training Loop...\")\n",
    "print(f\"Effective Batch Size: {BATCH_SIZE * GRAD_ACCUMULATION_STEPS}\")\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for step, batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        logits = outputs.logits.squeeze(-1) \n",
    "        \n",
    "        loss = loss_fn(logits, batch['labels'])\n",
    "        loss = loss / GRAD_ACCUMULATION_STEPS \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % GRAD_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        current_loss = loss.item() * GRAD_ACCUMULATION_STEPS\n",
    "        total_loss += current_loss\n",
    "        progress_bar.set_postfix({'loss': current_loss})\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    print(f\"Validating Epoch {epoch+1}...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            logits = outputs.logits.squeeze(-1)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            predictions = (probs > 0.5).float()\n",
    "            val_correct += (predictions == batch['labels']).sum().item()\n",
    "            val_total += len(batch['labels'])\n",
    "            \n",
    "    val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "    print(f\"Epoch {epoch+1} Finished | Train Loss: {avg_train_loss:.4f} | Val Acc: {val_acc:.2%}\")\n",
    "\n",
    "    print(\"Saving Checkpoint...\")\n",
    "    \n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(\"Checkpoint Saved.\\n\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b4edd",
   "metadata": {},
   "source": [
    "# Step 6-1: testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec88e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/SSD/llm-projects/envs/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on: mps\n",
      "Loading Tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /Volumes/SSD/llm-projects/hugging_face/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trained Adapter from ./verifier_lora_adapter...\n",
      "Loading Test Data from verifier_dataset_test.json...\n",
      "⚠️ DEBUG MODE: Evaluating only the first 100 questions.\n",
      "Loaded 100 test questions.\n",
      "Starting Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:24<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EVALUATION REPORT\n",
      "============================================================\n",
      "Total Questions Evaluated: 100\n",
      "Base Model Accuracy (Greedy):   44.00%  (Random/First guess)\n",
      "Verifier Accuracy (Best-of-N):  64.00%   (Your Trained Model)\n",
      "Oracle Accuracy (Theoretical):  92.00%   (Perfect Selection)\n",
      "------------------------------------------------------------\n",
      "Verifier Improvement: +20.00%\n",
      "✅ SUCCESS: Your Verifier is helping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "BASE_MODEL = \"/Volumes/SSD/llm-projects/hugging_face/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca\"\n",
    "ADAPTER_PATH = \"./verifier_lora\"\n",
    "TEST_DATA_PATH = \"verifier_dataset_test.json\" \n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8 \n",
    "\n",
    "# Set to 100 to check only the first 100 questions. Set to None for full test.\n",
    "DEBUG_TEST_SIZE = 100 \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Running evaluation on: {device}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Load Model & Adapter\n",
    "# ==============================================================================\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading Base Model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=1,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Loading Trained Adapter from {ADAPTER_PATH}...\")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "model.eval() \n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Load & Slice Test Data\n",
    "# ==============================================================================\n",
    "print(f\"Loading Test Data from {TEST_DATA_PATH}...\")\n",
    "if not os.path.exists(TEST_DATA_PATH):\n",
    "    print(f\"❌ Error: {TEST_DATA_PATH} not found. Please run generation script with ds['test'] first.\")\n",
    "    test_data = []\n",
    "else:\n",
    "    with open(TEST_DATA_PATH, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "# --- SLICING LOGIC ---\n",
    "if DEBUG_TEST_SIZE and len(test_data) > DEBUG_TEST_SIZE:\n",
    "    print(f\"⚠️ DEBUG MODE: Evaluating only the first {DEBUG_TEST_SIZE} questions.\")\n",
    "    test_data = test_data[:DEBUG_TEST_SIZE]\n",
    "else:\n",
    "    print(f\"Evaluating full dataset: {len(test_data)} questions.\")\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test questions.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Evaluation Loop (Best-of-N)\n",
    "# ==============================================================================\n",
    "stats = {\n",
    "    \"total\": 0,\n",
    "    \"correct_base\": 0,      \n",
    "    \"correct_verifier\": 0,  \n",
    "    \"correct_oracle\": 0     \n",
    "}\n",
    "\n",
    "print(\"Starting Evaluation...\")\n",
    "\n",
    "for entry in tqdm(test_data):\n",
    "    question = entry['question']\n",
    "    answers = entry['answers']\n",
    "    labels = entry['answer_labels'] \n",
    "    \n",
    "    if not answers: continue\n",
    "    \n",
    "    inputs_text = [f\"Question: {question}\\nAnswer: {ans}\" for ans in answers]\n",
    "    \n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(inputs_text), BATCH_SIZE):\n",
    "            batch_text = inputs_text[i : i + BATCH_SIZE]\n",
    "            \n",
    "            encodings = tokenizer(\n",
    "                batch_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=MAX_LENGTH\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**encodings)\n",
    "            logits = outputs.logits.squeeze(-1)\n",
    "            batch_scores = torch.sigmoid(logits).cpu().numpy().tolist()\n",
    "            scores.extend(batch_scores)\n",
    "            \n",
    "    best_idx = np.argmax(scores)\n",
    "    \n",
    "    stats[\"total\"] += 1\n",
    "    \n",
    "    # Metric 1: Base Model (Answer #0 is usually considered the 'greedy' or first sampled output)\n",
    "    if labels[0] == 1:\n",
    "        stats[\"correct_base\"] += 1\n",
    "        \n",
    "    # Metric 2: Verifier (Best-of-N)\n",
    "    if labels[best_idx] == 1:\n",
    "        stats[\"correct_verifier\"] += 1\n",
    "        \n",
    "    # Metric 3: Oracle (Is there ANY correct answer in the list?)\n",
    "    if sum(labels) > 0:\n",
    "        stats[\"correct_oracle\"] += 1\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Final Report\n",
    "# ==============================================================================\n",
    "if stats[\"total\"] > 0:\n",
    "    total = stats[\"total\"]\n",
    "    acc_base = stats[\"correct_base\"] / total\n",
    "    acc_ver = stats[\"correct_verifier\"] / total\n",
    "    acc_oracle = stats[\"correct_oracle\"] / total\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL EVALUATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Questions Evaluated: {total}\")\n",
    "    print(f\"Base Model Accuracy (Greedy):   {acc_base:.2%}  (Random/First guess)\")\n",
    "    print(f\"Verifier Accuracy (Best-of-N):  {acc_ver:.2%}   (Your Trained Model)\")\n",
    "    print(f\"Oracle Accuracy (Theoretical):  {acc_oracle:.2%}   (Perfect Selection)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    improvement = acc_ver - acc_base\n",
    "    print(f\"Verifier Improvement: {improvement:+.2%}\")\n",
    "\n",
    "    if improvement > 0:\n",
    "        print(\"✅ SUCCESS: Your Verifier is helping!\")\n",
    "    else:\n",
    "        print(\"❌ FAILURE: Your Verifier is hurting or not learning.\")\n",
    "else:\n",
    "    print(\"No data processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69941c73",
   "metadata": {},
   "source": [
    "# Step 6-2: testing with abstention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78ccc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on: mps\n",
      "Loading Tokenizer...\n",
      "Loading Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /Volumes/SSD/llm-projects/hugging_face/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trained Adapter from ./verifier_lora_adapter...\n",
      "Loading Test Data from verifier_dataset_test.json...\n",
      "⚠️ DEBUG MODE: Evaluating only the first 100 questions.\n",
      "Loaded 100 test questions.\n",
      "Starting Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:58<00:00,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EVALUATION REPORT (With Abstention)\n",
      "============================================================\n",
      "Total Questions: 100\n",
      "Base Model Accuracy:    44.00%\n",
      "Oracle Accuracy:        92.00%\n",
      "Verifier Accuracy:      66.00%  <-- (Includes correct 'None' predictions)\n",
      "------------------------------------------------------------\n",
      "Verifier Improvement:   +22.00%\n",
      "------------------------------------------------------------\n",
      "Abstention Stats:\n",
      "  - Correctly Abstains: 3 times (Saved us from wrong answers)\n",
      "  - Wrongly Abstains:   8 times (Missed a good answer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "BASE_MODEL = \"/Volumes/SSD/llm-projects/hugging_face/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca\"\n",
    "ADAPTER_PATH = \"./verifier_lora\"\n",
    "TEST_DATA_PATH = \"verifier_dataset_test.json\" \n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8 \n",
    "\n",
    "# Set to None for full run, or 100 for quick test\n",
    "DEBUG_TEST_SIZE = 100 \n",
    "\n",
    "# Threshold for saying \"None of these are correct\"\n",
    "CONFIDENCE_THRESHOLD = 0.5 \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Running evaluation on: {device}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Load Model & Adapter\n",
    "# ==============================================================================\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading Base Model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=1,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Loading Trained Adapter from {ADAPTER_PATH}...\")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "model.eval() \n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Load & Slice Test Data\n",
    "# ==============================================================================\n",
    "print(f\"Loading Test Data from {TEST_DATA_PATH}...\")\n",
    "if not os.path.exists(TEST_DATA_PATH):\n",
    "    print(f\"❌ Error: {TEST_DATA_PATH} not found.\")\n",
    "    test_data = []\n",
    "else:\n",
    "    with open(TEST_DATA_PATH, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "if DEBUG_TEST_SIZE and len(test_data) > DEBUG_TEST_SIZE:\n",
    "    print(f\"⚠️ DEBUG MODE: Evaluating only the first {DEBUG_TEST_SIZE} questions.\")\n",
    "    test_data = test_data[:DEBUG_TEST_SIZE]\n",
    "else:\n",
    "    print(f\"Evaluating full dataset: {len(test_data)} questions.\")\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test questions.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Evaluation Loop (Best-of-N with Abstention)\n",
    "# ==============================================================================\n",
    "stats = {\n",
    "    \"total\": 0,\n",
    "    \"correct_base\": 0,      \n",
    "    \"correct_verifier\": 0,  \n",
    "    \"correct_oracle\": 0,\n",
    "    \"abstain_correct\": 0,   # Times verifier correctly said \"None\"\n",
    "    \"abstain_wrong\": 0      # Times verifier said \"None\" but there WAS a correct answer\n",
    "}\n",
    "\n",
    "print(\"Starting Evaluation...\")\n",
    "\n",
    "for entry in tqdm(test_data):\n",
    "    question = entry['question']\n",
    "    answers = entry['answers']\n",
    "    labels = entry['answer_labels'] # 1 = Correct, 0 = Wrong\n",
    "    \n",
    "    if not answers: continue\n",
    "    \n",
    "    # --- Prepare Inputs ---\n",
    "    inputs_text = [f\"Question: {question}\\nAnswer: {ans}\" for ans in answers]\n",
    "    \n",
    "    # --- Score Candidates ---\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(inputs_text), BATCH_SIZE):\n",
    "            batch_text = inputs_text[i : i + BATCH_SIZE]\n",
    "            \n",
    "            encodings = tokenizer(\n",
    "                batch_text, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=MAX_LENGTH\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**encodings)\n",
    "            logits = outputs.logits.squeeze(-1)\n",
    "            # Sigmoid gives 0.0 to 1.0 probability\n",
    "            batch_scores = torch.sigmoid(logits).cpu().numpy().tolist()\n",
    "            scores.extend(batch_scores)\n",
    "            \n",
    "    # --- Decision Logic (The New Part) ---\n",
    "    best_idx = np.argmax(scores)\n",
    "    best_score = scores[best_idx]\n",
    "    \n",
    "    # Is there actually ANY correct answer in the pile?\n",
    "    has_correct_answer = (sum(labels) > 0)\n",
    "    \n",
    "    # Metric 1: Base Model (Greedy)\n",
    "    if labels[0] == 1:\n",
    "        stats[\"correct_base\"] += 1\n",
    "        \n",
    "    # Metric 2: Oracle (Potential)\n",
    "    if has_correct_answer:\n",
    "        stats[\"correct_oracle\"] += 1\n",
    "\n",
    "    # Metric 3: Verifier (With Abstention)\n",
    "    if best_score < CONFIDENCE_THRESHOLD:\n",
    "        # Case A: Verifier abstains (\"None look right\")\n",
    "        if not has_correct_answer:\n",
    "            # CORRECT ABSTENTION: There truly were no correct answers.\n",
    "            stats[\"correct_verifier\"] += 1\n",
    "            stats[\"abstain_correct\"] += 1\n",
    "        else:\n",
    "            # FAILED ABSTENTION: There was a correct answer, but we missed it.\n",
    "            stats[\"abstain_wrong\"] += 1\n",
    "    else:\n",
    "        # Case B: Verifier picks an answer\n",
    "        if labels[best_idx] == 1:\n",
    "            stats[\"correct_verifier\"] += 1\n",
    "\n",
    "    stats[\"total\"] += 1\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Final Report\n",
    "# ==============================================================================\n",
    "if stats[\"total\"] > 0:\n",
    "    total = stats[\"total\"]\n",
    "    acc_base = stats[\"correct_base\"] / total\n",
    "    acc_ver = stats[\"correct_verifier\"] / total\n",
    "    acc_oracle = stats[\"correct_oracle\"] / total\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL EVALUATION REPORT (With Abstention)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Base Model Accuracy:    {acc_base:.2%}\")\n",
    "    print(f\"Oracle Accuracy:        {acc_oracle:.2%}\")\n",
    "    print(f\"Verifier Accuracy:      {acc_ver:.2%}  <-- (Includes correct 'None' predictions)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Verifier Improvement:   {acc_ver - acc_base:+.2%}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Abstention Stats:\")\n",
    "    print(f\"  - Correctly Abstains: {stats['abstain_correct']} times (Saved us from wrong answers)\")\n",
    "    print(f\"  - Wrongly Abstains:   {stats['abstain_wrong']} times (Missed a good answer)\")\n",
    "else:\n",
    "    print(\"No data processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Env (SSD Drive)",
   "language": "python",
   "name": "llm-env-ssd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
