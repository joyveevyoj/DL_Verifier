{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0000cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5a3d0",
   "metadata": {},
   "source": [
    "## Verifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80798215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathAnswerVerifier:\n",
    "    def __init__(self, model_name: str, device: torch.device,\n",
    "                 label_yes: str = \" y\", label_no: str = \" n\"):\n",
    "        \"\"\"\n",
    "        A simple verifier built on top of a decoder-only LLM\n",
    "        (e.g., Qwen / GPT-2 style AutoModelForCausalLM).\n",
    "\n",
    "        Given (question, answer), it estimates:\n",
    "            P(correct | question, answer) in (0, 1)\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name, e.g. \"Qwen/Qwen2.5-0.5B\" or \"gpt2\".\n",
    "            device: \"cuda\" / \"cpu\".\n",
    "            label_yes: text label representing \"correct\" (here: ' y').\n",
    "            label_no: text label representing \"incorrect\" (here: ' n').\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=\"auto\",\n",
    "        )\n",
    "\n",
    "        if device is not None:\n",
    "            self.model.to(device)\n",
    "\n",
    "        # Store labels (for training you will reuse them)\n",
    "        self.label_yes = label_yes\n",
    "        self.label_no = label_no\n",
    "\n",
    "        # Pre-tokenize yes/no label sequences\n",
    "        # IMPORTANT: We tokenize them separately to append them later by ID,\n",
    "        # avoiding string concatenation artifacts.\n",
    "        self.yes_ids = self.tokenizer(label_yes, add_special_tokens=False).input_ids\n",
    "        self.no_ids = self.tokenizer(label_no, add_special_tokens=False).input_ids\n",
    "\n",
    "        if len(self.yes_ids) == 0 or len(self.no_ids) == 0:\n",
    "            raise ValueError(\"Tokenizer produced empty ids for yes/no labels.\")\n",
    "\n",
    "    def build_prompt(self, question: str, answer: str) -> str:\n",
    "        \"\"\"\n",
    "        Build the verifier prompt.\n",
    "        IMPORTANT: This should be used consistently in both inference and training.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Answer: {answer}\\n\\n\"\n",
    "            f\"Is this answer correct? Answer y(Yes) or n(No).\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score(self, question: str, answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Return P(correct | question, answer) in (0, 1).\n",
    "\n",
    "        Optimized implementation:\n",
    "        1. Uses Batching (computes Yes and No in a single forward pass).\n",
    "        2. Handles Tokenization correctly by concatenating IDs instead of strings.\n",
    "        \"\"\"\n",
    "        # 1. Prepare Prompt IDs\n",
    "        prompt = self.build_prompt(question, answer)\n",
    "        # Add BOS token if the model expects it, but do not truncate here generally\n",
    "        context_enc = self.tokenizer(prompt, add_special_tokens=True)\n",
    "        context_ids = context_enc.input_ids\n",
    "\n",
    "        # 2. Prepare Sequences (Context + Label) via Tensor Concatenation\n",
    "        # We convert to tensor immediately to use efficient concatenation\n",
    "        device = self.model.device\n",
    "        ctx_tensor = torch.tensor(context_ids, dtype=torch.long, device=device)\n",
    "        yes_tensor = torch.tensor(self.yes_ids, dtype=torch.long, device=device)\n",
    "        no_tensor = torch.tensor(self.no_ids, dtype=torch.long, device=device)\n",
    "\n",
    "        # Create two sequences: [Context, label_yes] and [Context, label_no]\n",
    "        seq_yes = torch.cat([ctx_tensor, yes_tensor])\n",
    "        seq_no = torch.cat([ctx_tensor, no_tensor])\n",
    "\n",
    "        # 3. Batching and Padding\n",
    "        # Pad sequences to handle cases where 'label_yes' and 'label_no' differ in length\n",
    "        pad_val = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence([seq_yes, seq_no], batch_first=True, padding_value=pad_val)\n",
    "        attention_mask = (input_ids != pad_val).long()\n",
    "\n",
    "        # 4. Forward Pass (Batch Size = 2)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Shape: [2, seq_len, vocab_size]\n",
    "        # Use log_softmax for numerical stability\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # 5. Extract Log Probabilities for Labels\n",
    "        # Helper to sum log-probs of the label tokens given context\n",
    "        def get_label_logprob(batch_idx, label_len):\n",
    "            # The label starts immediately after the context\n",
    "            start_pos = len(context_ids)\n",
    "            end_pos = start_pos + label_len\n",
    "            total_logprob = 0.0\n",
    "            for _, pos in enumerate(range(start_pos, end_pos)):\n",
    "                target_token_id = input_ids[batch_idx, pos].item()\n",
    "                # To predict token at `pos`, we look at logits at `pos - 1`\n",
    "                token_logprob = log_probs[batch_idx, pos - 1, target_token_id].item()\n",
    "                total_logprob += token_logprob\n",
    "            return total_logprob\n",
    "\n",
    "        logp_yes = get_label_logprob(0, len(self.yes_ids))\n",
    "        logp_no = get_label_logprob(1, len(self.no_ids))\n",
    "\n",
    "        # 6. Normalize: P(Yes) = exp(Yes) / (exp(Yes) + exp(No))\n",
    "        max_logp = max(logp_yes, logp_no)\n",
    "        p_yes_score = math.exp(logp_yes - max_logp)\n",
    "        p_no_score = math.exp(logp_no - max_logp)\n",
    "        prob_correct = p_yes_score / (p_yes_score + p_no_score)\n",
    "        return float(prob_correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7e7b9",
   "metadata": {},
   "source": [
    "## (Optional) Test Verifier Inference (Score Correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f430b",
   "metadata": {},
   "source": [
    "### Copy Answer Generation Code from experiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4cfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n",
      "Loading GSM8K dataset...\n",
      "Dataset loaded: 1319 test examples, 7473 train examples\n",
      "Model loaded on device: mps:0\n",
      "============================================================\n",
      "DEVICE INFORMATION\n",
      "============================================================\n",
      "PyTorch version: 2.9.0\n",
      "Backend: MPS (Apple Silicon GPU)\n",
      "MPS is available and will be used as the device.\n",
      "\n",
      "Using device: mps\n",
      "\n",
      "Model device: mps:0\n",
      "\n",
      "All model parameters are on: mps:0\n",
      "============================================================\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "Reference Answer: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "Reference Final Answer: 18\n",
      "\n",
      "================================================================================\n",
      "Generating different answers...\n",
      "\n",
      "============================================================\n",
      "Generating answer 1/3...\n",
      "============================================================\n",
      "\n",
      "Generated Answer 1:\n",
      "------------------------------------------------------------\n",
      "To find out how much Janet makes at the farmers' market every day:\n",
      "\n",
      "1. **Eggs laid daily**: 16 eggs  \n",
      "2. **Eaten for breakfast**: 3 eggs  \n",
      "3. **Baked muffins**: 4 eggs  \n",
      "4. **Sells eggs at farmers' market**: 16 - 3 - 4 = **9 eggs per day**  \n",
      "5. **Sells eggs at $2 per egg**: $9 × 2 = **$18**\n",
      "\n",
      "**Answer:** $18.####\n",
      "------------------------------------------------------------\n",
      "Extracted Answer: 18\n",
      "\n",
      "============================================================\n",
      "Generating answer 2/3...\n",
      "============================================================\n",
      "\n",
      "Generated Answer 2:\n",
      "------------------------------------------------------------\n",
      "To find how much Janet makes at the farmers' market, we need to calculate:\n",
      "\n",
      "1. **Eggs laid per day**: 16 eggs  \n",
      "2. **Eggs eaten**: 3 for breakfast + 3 for baking = 6 eggs  \n",
      "3. **Eggs sold**: 16 - 6 = 10 eggs  \n",
      "4. **Eggs sold per day**: 10 eggs × $2 = $20\n",
      "\n",
      "### Final Answer:\n",
      "#### $20\n",
      "------------------------------------------------------------\n",
      "Extracted Answer: 20\n",
      "\n",
      "============================================================\n",
      "Generating answer 3/3...\n",
      "============================================================\n",
      "\n",
      "Generated Answer 3:\n",
      "------------------------------------------------------------\n",
      "To find how much Janet makes at the farmers' market:\n",
      "\n",
      "1. **Eggs laid per day**: 16 eggs  \n",
      "2. **Eggs eaten**: 3 for breakfast + 3 for baking = 6 eggs  \n",
      "3. **Eggs remaining**: 16 - 6 = 10 eggs  \n",
      "4. **Selling price per egg**: $2  \n",
      "5. **Total earnings**: 10 × $2 = **$20**\n",
      "\n",
      "Answer: #### 20\n",
      "------------------------------------------------------------\n",
      "Extracted Answer: 20\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION RESULTS:\n",
      "================================================================================\n",
      "\n",
      "Answer 1:\n",
      "Extracted value: 18\n",
      "Correct: True\n",
      "Response preview: To find out how much Janet makes at the farmers' market every day:\n",
      "\n",
      "1. **Eggs laid daily**: 16 eggs  \n",
      "2. **Eaten for breakfast**: 3 eggs  \n",
      "3. **Baked muffins**: 4 eggs  \n",
      "4. **Sells eggs at farmers' ma...\n",
      "\n",
      "Answer 2:\n",
      "Extracted value: 20\n",
      "Correct: False\n",
      "Response preview: To find how much Janet makes at the farmers' market, we need to calculate:\n",
      "\n",
      "1. **Eggs laid per day**: 16 eggs  \n",
      "2. **Eggs eaten**: 3 for breakfast + 3 for baking = 6 eggs  \n",
      "3. **Eggs sold**: 16 - 6 = ...\n",
      "\n",
      "Answer 3:\n",
      "Extracted value: 20\n",
      "Correct: False\n",
      "Response preview: To find how much Janet makes at the farmers' market:\n",
      "\n",
      "1. **Eggs laid per day**: 16 eggs  \n",
      "2. **Eggs eaten**: 3 for breakfast + 3 for baking = 6 eggs  \n",
      "3. **Eggs remaining**: 16 - 6 = 10 eggs  \n",
      "4. **Se...\n",
      "\n",
      "================================================================================\n",
      "Summary: 1/3 answers were correct\n",
      "Correct answer indices: [0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(ds['test'])} test examples, {len(ds['train'])} train examples\")\n",
    "print(f\"Model loaded on device: {model.device}\")\n",
    "\n",
    "# Check device information\n",
    "print(\"=\"*60)\n",
    "print(\"DEVICE INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Backend: CUDA (NVIDIA GPU)\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved:  {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# 2. 如果没有 CUDA，再检查 MPS（Apple Silicon GPU）\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Backend: MPS (Apple Silicon GPU)\")\n",
    "    print(\"MPS is available and will be used as the device.\")\n",
    "\n",
    "else:\n",
    "    print(\"No GPU backend available. Running on CPU.\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Move model to the selected device\n",
    "model.to(device)\n",
    "# Check model device\n",
    "print(f\"\\nModel device: {model.device}\")\n",
    "\n",
    "# Check which device each parameter is on (for distributed models)\n",
    "devices = set()\n",
    "for name, param in model.named_parameters():\n",
    "    devices.add(str(param.device))\n",
    "\n",
    "if len(devices) > 1:\n",
    "    print(f\"\\nModel is distributed across multiple devices: {devices}\")\n",
    "else:\n",
    "    print(f\"\\nAll model parameters are on: {list(devices)[0]}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Helper Functions for Answer Extraction, Verification, and Generation\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract the numerical answer from the text.\n",
    "    GSM8K answers typically end with #### followed by the number.\n",
    "    \"\"\"\n",
    "    # Try to find the answer after ####\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', text)\n",
    "    if match:\n",
    "        # Remove commas from the number\n",
    "        return match.group(1).replace(',', '')\n",
    "\n",
    "    # Fallback: try to find the last number in the text\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "\n",
    "    return None\n",
    "\n",
    "def check_answer_correct(generated_answer, reference_answer):\n",
    "    \"\"\"\n",
    "    Check if the generated answer matches the reference answer.\n",
    "    \"\"\"\n",
    "    gen = extract_answer(generated_answer)\n",
    "    ref = extract_answer(reference_answer)\n",
    "\n",
    "    if gen is None or ref is None:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Compare as floats to handle different formats\n",
    "        return abs(float(gen) - float(ref)) < 0.01\n",
    "    except:\n",
    "        return gen == ref\n",
    "\n",
    "def generate_answers(question, num_answers=10, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate multiple different answers to the same question using Qwen chat template.\n",
    "    \"\"\"\n",
    "    # Format the prompt with explicit instruction to use GSM8K format\n",
    "    prompt_text = f\"\"\"Question: {question}\n",
    "Answer: Let's solve this step by step concisely. End your answer with #### followed by the final numerical answer.\"\"\"\n",
    "\n",
    "    # Use Qwen chat template format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # Disable thinking mode for faster generation\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate multiple answers\n",
    "    answers = []\n",
    "    for i in range(num_answers):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Generating answer {i+1}/{num_answers}...\")\n",
    "        print('='*60)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode only the generated part (not the input prompt)\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "        generated_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        answers.append(generated_text)\n",
    "\n",
    "        # Print the result after each trial\n",
    "        print(f\"\\nGenerated Answer {i+1}:\")\n",
    "        print('-'*60)\n",
    "        print(generated_text )\n",
    "        print('-'*60)\n",
    "        extracted = extract_answer(generated_text)\n",
    "        print(f\"Extracted Answer: {extracted}\")\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Test with the first question from the test set\n",
    "test_example = ds['test'][0]\n",
    "print(f\"Question: {test_example['question']}\")\n",
    "print(f\"\\nReference Answer: {test_example['answer']}\")\n",
    "print(f\"Reference Final Answer: {extract_answer(test_example['answer'])}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# Generate answers for the first question\n",
    "print(\"Generating different answers...\")\n",
    "num_answers = 3\n",
    "generated_answers = generate_answers(test_example['question'], num_answers=num_answers)\n",
    "\n",
    "# Check which answers are correct\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correct_answers = []\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    extracted = extract_answer(answer)\n",
    "    is_correct = check_answer_correct(answer, test_example['answer'])\n",
    "\n",
    "    print(f\"\\nAnswer {i+1}:\")\n",
    "    print(f\"Extracted value: {extracted}\")\n",
    "    print(f\"Correct: {is_correct}\")\n",
    "    print(f\"Response preview: {answer[:200]}...\")\n",
    "\n",
    "    if is_correct:\n",
    "        correct_answers.append(i)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Summary: {len(correct_answers)}/{num_answers} answers were correct\")\n",
    "print(f\"Correct answer indices: {correct_answers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73725c23",
   "metadata": {},
   "source": [
    "### Score Answers using New Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0048f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading verifier model...\n",
      "Scoring all generated answers...\n",
      "Answer 1: Correctness Score = 0.1561, Correct = True\n",
      "Answer 2: Correctness Score = 0.4844, Correct = False\n",
      "Answer 3: Correctness Score = 0.3630, Correct = False\n",
      "\n",
      "================================================================================\n",
      "Best answer according to verifier: Answer 2\n",
      "Is the best answer correct? False\n",
      "Best answer: To find how much Janet makes at the farmers' market, we need to calculate:\n",
      "\n",
      "1. **Eggs laid per day**: 16 eggs  \n",
      "2. **Eggs eaten**: 3 for breakfast + 3 for baking = 6 eggs  \n",
      "3. **Eggs sold**: 16 - 6 = 10 eggs  \n",
      "4. **Eggs sold per day**: 10 eggs × $2 = $20\n",
      "\n",
      "### Final Answer:\n",
      "#### $20...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading verifier model...\")\n",
    "verifier_model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = model.device  # Use the same device as the main model\n",
    "verifier = MathAnswerVerifier(verifier_model_name, device)\n",
    "\n",
    "print(\"Scoring all generated answers...\")\n",
    "scores = []\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    score = verifier.score(test_example['question'], answer)\n",
    "    scores.append(score)\n",
    "    is_correct = check_answer_correct(answer, test_example['answer'])\n",
    "    print(f\"Answer {i+1}: Correctness Score = {score:.4f}, Correct = {is_correct}\")\n",
    "\n",
    "# Find the best answer according to the verifier\n",
    "best_idx = scores.index(max(scores))\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Best answer according to verifier: Answer {best_idx + 1}\")\n",
    "print(f\"Is the best answer correct? {check_answer_correct(generated_answers[best_idx], test_example['answer'])}\")\n",
    "print(f\"Best answer: {generated_answers[best_idx][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d637c",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240666c",
   "metadata": {},
   "source": [
    "### Dataset Class for Verifier Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20435aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerifierDataset(Dataset):\n",
    "    def __init__(self, raw_data, verifier, max_length: int = 512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_data: List of dicts. Each dict contains 'question', 'reference_answer',\n",
    "                      'answers' (list), and 'answer_labels' (list).\n",
    "            verifier: The verifier model wrapper.\n",
    "            max_length: Token limit.\n",
    "        \"\"\"\n",
    "        self.verifier = verifier\n",
    "        self.tokenizer = verifier.tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "\n",
    "        # Flatten the dataset with Data Augmentation\n",
    "        for entry in raw_data:\n",
    "            question = entry[\"question\"]\n",
    "\n",
    "            # 1. Get original answers and labels\n",
    "            answers = entry[\"answers\"]\n",
    "            labels = entry[\"answer_labels\"]\n",
    "\n",
    "            # 2. Get the reference answer\n",
    "            ref_answer = entry[\"reference_answer\"]\n",
    "\n",
    "            # 3. Build the complete list for training\n",
    "            # If a reference answer exists, append it to the end and add the corresponding label 1\n",
    "            if ref_answer is not None:\n",
    "                train_answers = answers + [ref_answer]\n",
    "                train_labels = labels + [1]\n",
    "            else:\n",
    "                print(\"Warning: No reference answer provided for question:\", question)\n",
    "                train_answers = answers\n",
    "                train_labels = labels\n",
    "\n",
    "            # 4. Flatten the dataset\n",
    "            # Split the structure of 1 Question -> N+1 Answers into N+1 independent samples\n",
    "            for ans, lbl in zip(train_answers, train_labels):\n",
    "                self.samples.append({\n",
    "                    \"question\": question,\n",
    "                    \"answer\": ans,\n",
    "                    \"label\": lbl\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.samples[idx]\n",
    "        q = ex[\"question\"]\n",
    "        a = ex[\"answer\"]\n",
    "        y = ex[\"label\"]  # 1 or 0\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = self.verifier.build_prompt(q, a)\n",
    "\n",
    "        # Label mapping\n",
    "        target_text = self.verifier.label_yes if y == 1 else self.verifier.label_no\n",
    "\n",
    "        # 1. Tokenize Context\n",
    "        context_enc = self.tokenizer(\n",
    "            prompt,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=False\n",
    "        )\n",
    "        context_ids = context_enc.input_ids\n",
    "\n",
    "        # 2. Tokenize Label\n",
    "        target_enc = self.tokenizer(\n",
    "            target_text,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False\n",
    "        )\n",
    "        target_ids = target_enc.input_ids\n",
    "        target_ids += [self.tokenizer.eos_token_id]\n",
    "\n",
    "        # 3. Concatenate\n",
    "        full_ids = context_ids + target_ids\n",
    "\n",
    "        # 4. Truncate\n",
    "        if len(full_ids) > self.max_length:\n",
    "            full_ids = full_ids[-self.max_length:]\n",
    "            new_context_len = len(full_ids) - len(target_ids)\n",
    "        else:\n",
    "            new_context_len = len(context_ids)\n",
    "\n",
    "        input_ids = torch.tensor(full_ids, dtype=torch.long)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        if new_context_len > 0:\n",
    "            labels[:new_context_len] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91765c",
   "metadata": {},
   "source": [
    "### Function for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bc906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_verifier(verifier, data, epochs=1, batch_size=4, lr=1e-5, warmup_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Finetune the verifier with a Learning Rate Scheduler.\n",
    "    \"\"\"\n",
    "    # The Dataset initialization automatically handles the logic of appending the reference_answer\n",
    "    dataset = VerifierDataset(data, verifier)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [b[\"input_ids\"] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=verifier.tokenizer.pad_token_id or verifier.tokenizer.eos_token_id\n",
    "        ),\n",
    "        \"attention_mask\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [b[\"attention_mask\"] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        ),\n",
    "        \"labels\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [b[\"labels\"] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=-100\n",
    "        ),\n",
    "    })\n",
    "\n",
    "    model = verifier.model\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    num_warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset size (augmented & flattened): {len(dataset)}\")\n",
    "    print(f\"Training for {total_steps} steps with {num_warmup_steps} warmup steps.\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            if (step + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}, step {step+1}, loss = {total_loss / (step+1):.4f}, lr = {current_lr:.2e}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished, avg loss = {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    return verifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123ec02",
   "metadata": {},
   "source": [
    "### Finetuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d0d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading model gpt2...\n",
      "\n",
      "================================================================================\n",
      "PRE-TRAINING TEST\n",
      "================================================================================\n",
      "Q: What is 5 + 5?\n",
      "A: The answer is 10. (Correct) -> Score: 0.4030\n",
      "A: The answer is 12. (Wrong)   -> Score: 0.3993\n",
      "\n",
      "================================================================================\n",
      "START FINE-TUNING\n",
      "================================================================================\n",
      "Dataset size (augmented & flattened): 65\n",
      "Training for 99 steps with 9 warmup steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished, avg loss = 1.5966\n",
      "Epoch 2 finished, avg loss = 0.5632\n",
      "Epoch 3 finished, avg loss = 0.3404\n",
      "\n",
      "================================================================================\n",
      "POST-TRAINING TEST\n",
      "================================================================================\n",
      "Q: What is 5 + 5?\n",
      "A: The answer is 10. (Correct) -> Score: 0.6403\n",
      "A: The answer is 12. (Wrong)   -> Score: 0.6341\n",
      "\n",
      "Testing generalization (Unseen Data):\n",
      "Q: What is 6 * 2?\n",
      "A: It is 12. -> 0.6514\n",
      "A: It is 100. -> 0.5871\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Device (Prioritize GPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Initialize Verifier\n",
    "# use gpt2 as a demo because it's small and fast.\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading model {model_name}...\")\n",
    "verifier = MathAnswerVerifier(\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    label_yes=\" y\",\n",
    "    label_no=\" n\"\n",
    ")\n",
    "\n",
    "# Some Decoder-only models (e.g., GPT2) don't have a pad_token by default; manually set it to eos_token\n",
    "if verifier.tokenizer.pad_token is None:\n",
    "    verifier.tokenizer.pad_token = verifier.tokenizer.eos_token\n",
    "    verifier.model.config.pad_token_id = verifier.model.config.eos_token_id\n",
    "\n",
    "# 3. Construct a Complex Dummy Dataset\n",
    "# The updated VerifierDataset expects:\n",
    "# - 'question': str\n",
    "# - 'answers': List[str]\n",
    "# - 'answer_labels': List[int] (0 or 1)\n",
    "# - 'reference_answer': str (will be automatically added as a positive sample)\n",
    "raw_train_data = [\n",
    "    {\n",
    "        \"question\": \"What is 3 + 3?\",\n",
    "        \"answers\": [\n",
    "            \"The answer is 5.\",    # Wrong\n",
    "            \"It is 6.\",            # Correct\n",
    "            \"3 + 3 equals 7.\"      # Wrong\n",
    "        ],\n",
    "        \"answer_labels\": [0, 1, 0],\n",
    "        \"reference_answer\": \"6\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Solve: 10 - 2\",\n",
    "        \"answers\": [\n",
    "            \"10 minus 2 is 8.\",    # Correct\n",
    "            \"The result is 0.\"     # Wrong\n",
    "        ],\n",
    "        \"answer_labels\": [1, 0],\n",
    "        \"reference_answer\": \"8\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Calculate 4 * 2\",\n",
    "        \"answers\": [\n",
    "            \"4 * 2 = 6\",           # Wrong\n",
    "            \"It is 8\"              # Correct\n",
    "        ],\n",
    "        \"answer_labels\": [0, 1],\n",
    "        \"reference_answer\": \"8\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is 10 / 2?\",\n",
    "        \"answers\": [\n",
    "            \"5\",                   # Correct\n",
    "            \"2\"                    # Wrong\n",
    "        ],\n",
    "        \"answer_labels\": [1, 0],\n",
    "        \"reference_answer\": \"5\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Duplicate data to simulate a larger dataset for the scheduler\n",
    "# (In real scenarios, use more diverse data)\n",
    "train_data = raw_train_data * 5\n",
    "random.shuffle(train_data)\n",
    "\n",
    "# 4. Test Case Definition (Unseen Data)\n",
    "test_q = \"What is 5 + 5?\"\n",
    "test_a_correct = \"The answer is 10.\"\n",
    "test_a_wrong = \"The answer is 12.\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRE-TRAINING TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Assuming 'verifier' is already initialized from your MathAnswerVerifier class\n",
    "score_correct = verifier.score(test_q, test_a_correct)\n",
    "score_wrong = verifier.score(test_q, test_a_wrong)\n",
    "\n",
    "print(f\"Q: {test_q}\")\n",
    "print(f\"A: {test_a_correct} (Correct) -> Score: {score_correct:.4f}\")\n",
    "print(f\"A: {test_a_wrong} (Wrong)   -> Score: {score_wrong:.4f}\")\n",
    "\n",
    "# 5. Start Fine-tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"START FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: The dataset class will automatically:\n",
    "# 1. Append 'reference_answer' to 'answers'\n",
    "# 2. Append 1 to 'answer_labels'\n",
    "# 3. Flatten the list so one question becomes multiple training samples\n",
    "verifier = finetune_verifier(\n",
    "    verifier,\n",
    "    train_data,       # Passing the complex structure directly\n",
    "    epochs=3,\n",
    "    batch_size=2,     # Smaller batch size for this dummy example\n",
    "    lr=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "# 6. Post-training Test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POST-TRAINING TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "score_correct_post = verifier.score(test_q, test_a_correct)\n",
    "score_wrong_post = verifier.score(test_q, test_a_wrong)\n",
    "\n",
    "print(f\"Q: {test_q}\")\n",
    "print(f\"A: {test_a_correct} (Correct) -> Score: {score_correct_post:.4f}\")\n",
    "print(f\"A: {test_a_wrong} (Wrong)   -> Score: {score_wrong_post:.4f}\")\n",
    "\n",
    "# 7. Test Generalization (Another Unseen Case)\n",
    "print(\"\\nTesting generalization (Unseen Data):\")\n",
    "unseen_q = \"What is 6 * 2?\"\n",
    "unseen_a_corr = \"It is 12.\"\n",
    "unseen_a_wrong = \"It is 100.\"\n",
    "\n",
    "s_corr = verifier.score(unseen_q, unseen_a_corr)\n",
    "s_wrong = verifier.score(unseen_q, unseen_a_wrong)\n",
    "print(f\"Q: {unseen_q}\")\n",
    "print(f\"A: {unseen_a_corr} -> {s_corr:.4f}\")\n",
    "print(f\"A: {unseen_a_wrong} -> {s_wrong:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-verifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
