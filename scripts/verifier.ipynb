{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0000cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5a3d0",
   "metadata": {},
   "source": [
    "## Verifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80798215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathAnswerVerifier:\n",
    "    def __init__(self, model_name: str, device: torch.device,\n",
    "                 label_yes: str = \" y\", label_no: str = \" n\"):\n",
    "        \"\"\"\n",
    "        A simple verifier built on top of a decoder-only LLM\n",
    "        (e.g., Qwen / GPT-2 style AutoModelForCausalLM).\n",
    "\n",
    "        Given (question, answer), it estimates:\n",
    "            P(correct | question, answer) in (0, 1)\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name, e.g. \"Qwen/Qwen2.5-0.5B\" or \"gpt2\".\n",
    "            device: \"cuda\" / \"cpu\".\n",
    "            label_yes: text label representing \"correct\" (here: ' y').\n",
    "            label_no: text label representing \"incorrect\" (here: ' n').\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "        )\n",
    "\n",
    "        if device is not None:\n",
    "            self.model.to(device)\n",
    "\n",
    "        # Store labels (for training you will reuse them)\n",
    "        self.label_yes = label_yes\n",
    "        self.label_no = label_no\n",
    "\n",
    "        # Pre-tokenize yes/no label sequences\n",
    "        # IMPORTANT: We tokenize them separately to append them later by ID, \n",
    "        # avoiding string concatenation artifacts.\n",
    "        self.yes_ids = self.tokenizer(label_yes, add_special_tokens=False).input_ids\n",
    "        self.no_ids = self.tokenizer(label_no, add_special_tokens=False).input_ids\n",
    "\n",
    "        if len(self.yes_ids) == 0 or len(self.no_ids) == 0:\n",
    "            raise ValueError(\"Tokenizer produced empty ids for yes/no labels.\")\n",
    "\n",
    "    def build_prompt(self, question: str, answer: str) -> str:\n",
    "        \"\"\"\n",
    "        Build the verifier prompt.\n",
    "        IMPORTANT: This should be used consistently in both inference and training.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Answer: {answer}\\n\\n\"\n",
    "            f\"Is this answer correct? Answer y(Yes) or n(No).\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score(self, question: str, answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Return P(correct | question, answer) in (0, 1).\n",
    "        \n",
    "        Optimized implementation:\n",
    "        1. Uses Batching (computes Yes and No in a single forward pass).\n",
    "        2. Handles Tokenization correctly by concatenating IDs instead of strings.\n",
    "        \"\"\"\n",
    "        # 1. Prepare Prompt IDs\n",
    "        prompt = self.build_prompt(question, answer)\n",
    "        # Add BOS token if the model expects it, but do not truncate here generally\n",
    "        context_enc = self.tokenizer(prompt, add_special_tokens=True)\n",
    "        context_ids = context_enc.input_ids\n",
    "        \n",
    "        # 2. Prepare Sequences (Context + Label) via Tensor Concatenation\n",
    "        # We convert to tensor immediately to use efficient concatenation\n",
    "        device = self.model.device\n",
    "        ctx_tensor = torch.tensor(context_ids, dtype=torch.long, device=device)\n",
    "        yes_tensor = torch.tensor(self.yes_ids, dtype=torch.long, device=device)\n",
    "        no_tensor = torch.tensor(self.no_ids, dtype=torch.long, device=device)\n",
    "\n",
    "        # Create two sequences: [Context, label_yes] and [Context, label_no]\n",
    "        seq_yes = torch.cat([ctx_tensor, yes_tensor])\n",
    "        seq_no = torch.cat([ctx_tensor, no_tensor])\n",
    "\n",
    "        # 3. Batching and Padding\n",
    "        # Pad sequences to handle cases where 'label_yes' and 'label_no' differ in length\n",
    "        pad_val = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence([seq_yes, seq_no], batch_first=True, padding_value=pad_val)\n",
    "        attention_mask = (input_ids != pad_val).long()\n",
    "\n",
    "        # 4. Forward Pass (Batch Size = 2)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Shape: [2, seq_len, vocab_size]\n",
    "        # Use log_softmax for numerical stability\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # 5. Extract Log Probabilities for Labels\n",
    "        # Helper to sum log-probs of the label tokens given context\n",
    "        def get_label_logprob(batch_idx, label_len):\n",
    "            # The label starts immediately after the context\n",
    "            start_pos = len(context_ids)\n",
    "            end_pos = start_pos + label_len\n",
    "            total_logprob = 0.0\n",
    "            for _, pos in enumerate(range(start_pos, end_pos)):\n",
    "                target_token_id = input_ids[batch_idx, pos].item()\n",
    "                # To predict token at `pos`, we look at logits at `pos - 1`\n",
    "                token_logprob = log_probs[batch_idx, pos - 1, target_token_id].item()\n",
    "                total_logprob += token_logprob\n",
    "            return total_logprob\n",
    "\n",
    "        logp_yes = get_label_logprob(0, len(self.yes_ids))\n",
    "        logp_no = get_label_logprob(1, len(self.no_ids))\n",
    "\n",
    "        # 6. Normalize: P(Yes) = exp(Yes) / (exp(Yes) + exp(No))\n",
    "        max_logp = max(logp_yes, logp_no)\n",
    "        p_yes_score = math.exp(logp_yes - max_logp)\n",
    "        p_no_score = math.exp(logp_no - max_logp)\n",
    "        prob_correct = p_yes_score / (p_yes_score + p_no_score)\n",
    "        return float(prob_correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7e7b9",
   "metadata": {},
   "source": [
    "## (Optional) Test Verifier Inference (Score Correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f430b",
   "metadata": {},
   "source": [
    "### Copy Answer Generation Code from experiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(ds['test'])} test examples, {len(ds['train'])} train examples\")\n",
    "print(f\"Model loaded on device: {model.device}\")\n",
    "\n",
    "# Check device information\n",
    "print(\"=\"*60)\n",
    "print(\"DEVICE INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Memory information\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Check model device\n",
    "print(f\"\\nModel device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "\n",
    "# Check which device each parameter is on (for distributed models)\n",
    "devices = set()\n",
    "for name, param in model.named_parameters():\n",
    "    devices.add(str(param.device))\n",
    "\n",
    "if len(devices) > 1:\n",
    "    print(f\"\\nModel is distributed across multiple devices: {devices}\")\n",
    "else:\n",
    "    print(f\"\\nAll model parameters are on: {list(devices)[0]}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Helper Functions for Answer Extraction, Verification, and Generation\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract the numerical answer from the text.\n",
    "    GSM8K answers typically end with #### followed by the number.\n",
    "    \"\"\"\n",
    "    # Try to find the answer after ####\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', text)\n",
    "    if match:\n",
    "        # Remove commas from the number\n",
    "        return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Fallback: try to find the last number in the text\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def check_answer_correct(generated_answer, reference_answer):\n",
    "    \"\"\"\n",
    "    Check if the generated answer matches the reference answer.\n",
    "    \"\"\"\n",
    "    gen = extract_answer(generated_answer)\n",
    "    ref = extract_answer(reference_answer)\n",
    "    \n",
    "    if gen is None or ref is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Compare as floats to handle different formats\n",
    "        return abs(float(gen) - float(ref)) < 0.01\n",
    "    except:\n",
    "        return gen == ref\n",
    "\n",
    "def generate_answers(question, num_answers=10, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate multiple different answers to the same question using Qwen chat template.\n",
    "    \"\"\"\n",
    "    # Format the prompt with explicit instruction to use GSM8K format\n",
    "    prompt_text = f\"\"\"Question: {question}\n",
    "Answer: Let's solve this step by step concisely. End your answer with #### followed by the final numerical answer.\"\"\"\n",
    "    \n",
    "    # Use Qwen chat template format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # Disable thinking mode for faster generation\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate multiple answers\n",
    "    answers = []\n",
    "    for i in range(num_answers):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Generating answer {i+1}/{num_answers}...\")\n",
    "        print('='*60)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part (not the input prompt)\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "        generated_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "        \n",
    "        answers.append(generated_text)\n",
    "        \n",
    "        # Print the result after each trial\n",
    "        print(f\"\\nGenerated Answer {i+1}:\")\n",
    "        print('-'*60)\n",
    "        print(generated_text )\n",
    "        print('-'*60)\n",
    "        extracted = extract_answer(generated_text)\n",
    "        print(f\"Extracted Answer: {extracted}\")\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Test with the first question from the test set\n",
    "test_example = ds['test'][0]\n",
    "print(f\"Question: {test_example['question']}\")\n",
    "print(f\"\\nReference Answer: {test_example['answer']}\")\n",
    "print(f\"Reference Final Answer: {extract_answer(test_example['answer'])}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# Generate answers for the first question\n",
    "print(\"Generating different answers...\")\n",
    "num_answers = 3\n",
    "generated_answers = generate_answers(test_example['question'], num_answers=num_answers)\n",
    "\n",
    "# Check which answers are correct\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correct_answers = []\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    extracted = extract_answer(answer)\n",
    "    is_correct = check_answer_correct(answer, test_example['answer'])\n",
    "    \n",
    "    print(f\"\\nAnswer {i+1}:\")\n",
    "    print(f\"Extracted value: {extracted}\")\n",
    "    print(f\"Correct: {is_correct}\")\n",
    "    print(f\"Response preview: {answer[:200]}...\")\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_answers.append(i)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Summary: {len(correct_answers)}/{num_answers} answers were correct\")\n",
    "print(f\"Correct answer indices: {correct_answers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73725c23",
   "metadata": {},
   "source": [
    "### Score Answers using New Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading verifier model...\")\n",
    "verifier_model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = model.device  # Use the same device as the main model\n",
    "verifier = MathAnswerVerifier(verifier_model_name, device)\n",
    "\n",
    "print(\"Scoring all generated answers...\")\n",
    "scores = []\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    score = verifier.score(test_example['question'], answer)\n",
    "    scores.append(score)\n",
    "    is_correct = check_answer_correct(answer, test_example['answer'])\n",
    "    print(f\"Answer {i+1}: Correctness Score = {score:.4f}, Correct = {is_correct}\")\n",
    "\n",
    "# Find the best answer according to the verifier\n",
    "best_idx = scores.index(max(scores))\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Best answer according to verifier: Answer {best_idx + 1}\")\n",
    "print(f\"Is the best answer correct? {check_answer_correct(generated_answers[best_idx], test_example['answer'])}\")\n",
    "print(f\"Best answer: {generated_answers[best_idx][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d637c",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240666c",
   "metadata": {},
   "source": [
    "### Dataset Class for Verifier Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20435aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerifierDataset(Dataset):\n",
    "    def __init__(self, examples, verifier: MathAnswerVerifier, max_length: int = 512):\n",
    "        self.examples = examples\n",
    "        self.verifier = verifier\n",
    "        self.tokenizer = verifier.tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        q = ex[\"question\"]\n",
    "        a = ex[\"answer\"]\n",
    "        y = ex[\"label\"]  # 1 or 0\n",
    "\n",
    "        prompt = self.verifier.build_prompt(q, a)\n",
    "\n",
    "        # Choose \"y\" or \"n\" according to label\n",
    "        target_text = self.verifier.label_yes if y == 1 else self.verifier.label_no\n",
    "\n",
    "        # 1. Tokenize Context (add special tokens like BOS)\n",
    "        context_enc = self.tokenizer(\n",
    "            prompt, \n",
    "            add_special_tokens=True, \n",
    "            return_attention_mask=False\n",
    "        )\n",
    "        context_ids = context_enc.input_ids\n",
    "\n",
    "        # 2. Tokenize Label (NO special tokens)\n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, \n",
    "            add_special_tokens=False, \n",
    "            return_attention_mask=False\n",
    "        )\n",
    "        target_ids = target_enc.input_ids\n",
    "        target_ids += [self.tokenizer.eos_token_id]\n",
    "\n",
    "        # 3. Concatenate\n",
    "        full_ids = context_ids + target_ids\n",
    "\n",
    "        # 4. Truncate\n",
    "        if len(full_ids) > self.max_length:\n",
    "            full_ids = full_ids[-self.max_length:]\n",
    "            new_context_len = len(full_ids) - len(target_ids)\n",
    "        else:\n",
    "            new_context_len = len(context_ids)\n",
    "\n",
    "        input_ids = torch.tensor(full_ids, dtype=torch.long)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        labels = input_ids.clone()\n",
    "        if new_context_len > 0:\n",
    "            labels[:new_context_len] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91765c",
   "metadata": {},
   "source": [
    "### Function for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_verifier(verifier: MathAnswerVerifier, data, epochs=1, batch_size=4, lr=1e-5, warmup_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Finetune the verifier with a Learning Rate Scheduler.\n",
    "    \"\"\"\n",
    "    dataset = VerifierDataset(data, verifier)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [b[\"input_ids\"] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=verifier.tokenizer.pad_token_id or verifier.tokenizer.eos_token_id\n",
    "        ),\n",
    "        \"attention_mask\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [b[\"attention_mask\"] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        ),\n",
    "        \"labels\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [b[\"labels\"] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=-100\n",
    "        ),\n",
    "    })\n",
    "\n",
    "    model = verifier.model\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Scheduler Setup\n",
    "    # Calculate total training steps\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    # Calculate warmup steps (usually 3% to 10% of total steps)\n",
    "    num_warmup_steps = int(total_steps * warmup_ratio)\n",
    "    # Create the scheduler\n",
    "    # It linearly increases LR from 0 to `lr` during warmup, \n",
    "    # then linearly decreases it to 0 by the end of training.\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    print(f\"Training for {total_steps} steps with {num_warmup_steps} warmup steps.\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            if (step + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}, step {step+1}, loss = {total_loss / (step+1):.4f}, lr = {current_lr:.2e}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished, avg loss = {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    return verifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123ec02",
   "metadata": {},
   "source": [
    "### Finetuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 21:47:19.653948: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-23 21:47:19.682239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-23 21:47:20.576605: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRE-TRAINING TEST\n",
      "================================================================================\n",
      "Q: 2 + 2 = ?\n",
      "A: 4 (Correct) -> Score: 0.4586\n",
      "A: 5 (Wrong)   -> Score: 0.4260\n",
      "\n",
      "================================================================================\n",
      "START FINE-TUNING\n",
      "================================================================================\n",
      "Training for 40 steps with 4 warmup steps.\n",
      "Epoch 1 finished, avg loss = 3.7910\n",
      "Epoch 2 finished, avg loss = 0.9050\n",
      "Epoch 3 finished, avg loss = 0.8529\n",
      "Epoch 4 finished, avg loss = 0.6503\n",
      "Epoch 5 finished, avg loss = 0.5194\n",
      "\n",
      "================================================================================\n",
      "POST-TRAINING TEST\n",
      "================================================================================\n",
      "Q: 2 + 2 = ?\n",
      "A: 4 (Correct) -> Score: 0.6832\n",
      "A: 5 (Wrong)   -> Score: 0.6585\n",
      "\n",
      "Testing generalization (Unseen Data):\n",
      "Q: 5 + 5 = ?, A: 10 -> 0.6299\n",
      "Q: 5 + 5 = ?, A: 12 -> 0.6061\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Device (Prioritize GPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Initialize Verifier\n",
    "# use gpt2 as a demo because it's small and fast.\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading model {model_name}...\")\n",
    "verifier = MathAnswerVerifier(\n",
    "    model_name=model_name, \n",
    "    device=device,\n",
    "    label_yes=\" y\",\n",
    "    label_no=\" n\"\n",
    ")\n",
    "\n",
    "# Some Decoder-only models (e.g., GPT2) don't have a pad_token by default; manually set it to eos_token\n",
    "if verifier.tokenizer.pad_token is None:\n",
    "    verifier.tokenizer.pad_token = verifier.tokenizer.eos_token\n",
    "    verifier.model.config.pad_token_id = verifier.model.config.eos_token_id\n",
    "\n",
    "# 3. Construct a Tiny Dataset (Training Data)\n",
    "# Includes simple math problems, both correct (label=1) and incorrect (label=0)\n",
    "train_data = [\n",
    "    {\"question\": \"1 + 1 = ?\", \"answer\": \"2\", \"label\": 1},\n",
    "    {\"question\": \"1 + 1 = ?\", \"answer\": \"3\", \"label\": 0},\n",
    "    {\"question\": \"2 * 3 = ?\", \"answer\": \"6\", \"label\": 1},\n",
    "    {\"question\": \"2 * 3 = ?\", \"answer\": \"8\", \"label\": 0},\n",
    "    {\"question\": \"10 - 5 = ?\", \"answer\": \"5\", \"label\": 1},\n",
    "    {\"question\": \"10 - 5 = ?\", \"answer\": \"4\", \"label\": 0},\n",
    "    {\"question\": \"3 + 3 = ?\", \"answer\": \"6\", \"label\": 1},\n",
    "    {\"question\": \"3 + 3 = ?\", \"answer\": \"9\", \"label\": 0},\n",
    "]\n",
    "\n",
    "# Duplicate data to simulate a slightly larger epoch\n",
    "train_data = train_data * 4  # Total 32 samples\n",
    "random.shuffle(train_data)\n",
    "\n",
    "# 4. Test Case Definition\n",
    "test_q = \"2 + 2 = ?\"\n",
    "test_a_correct = \"4\"\n",
    "test_a_wrong = \"5\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRE-TRAINING TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "score_correct = verifier.score(test_q, test_a_correct)\n",
    "score_wrong = verifier.score(test_q, test_a_wrong)\n",
    "\n",
    "print(f\"Q: {test_q}\")\n",
    "print(f\"A: {test_a_correct} (Correct) -> Score: {score_correct:.4f}\")\n",
    "print(f\"A: {test_a_wrong} (Wrong)   -> Score: {score_wrong:.4f}\")\n",
    "\n",
    "# At this point, the model likely hasn't learned to output 'y' or 'n', \n",
    "# so scores might be random (around 0.5).\n",
    "\n",
    "# 5. Start Fine-tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"START FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train for 5 epochs with a slightly higher LR for demonstration purposes\n",
    "verifier = finetune_verifier(\n",
    "    verifier, \n",
    "    train_data, \n",
    "    epochs=5, \n",
    "    batch_size=4, \n",
    "    lr=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "# 6. Post-training Test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POST-TRAINING TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "score_correct_post = verifier.score(test_q, test_a_correct)\n",
    "score_wrong_post = verifier.score(test_q, test_a_wrong)\n",
    "\n",
    "print(f\"Q: {test_q}\")\n",
    "print(f\"A: {test_a_correct} (Correct) -> Score: {score_correct_post:.4f}\")\n",
    "print(f\"A: {test_a_wrong} (Wrong)   -> Score: {score_wrong_post:.4f}\")\n",
    "\n",
    "# 7. Test Generalization (Unseen Data)\n",
    "print(\"\\nTesting generalization (Unseen Data):\")\n",
    "unseen_q = \"5 + 5 = ?\"\n",
    "unseen_a_corr = \"10\"\n",
    "unseen_a_wrong = \"12\"\n",
    "\n",
    "s_corr = verifier.score(unseen_q, unseen_a_corr)\n",
    "s_wrong = verifier.score(unseen_q, unseen_a_wrong)\n",
    "print(f\"Q: {unseen_q}, A: {unseen_a_corr} -> {s_corr:.4f}\")\n",
    "print(f\"Q: {unseen_q}, A: {unseen_a_wrong} -> {s_wrong:.4f}\")\n",
    "\n",
    "# 8. Save Model (Optional)\n",
    "# verifier.model.save_pretrained(\"./my_math_verifier\")\n",
    "# verifier.tokenizer.save_pretrained(\"./my_math_verifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
